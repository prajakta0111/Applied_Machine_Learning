{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate, iterations, \n",
    "               fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "        self.min = 9999\n",
    "        \n",
    "    def mse(self, y_actual, y_pred):\n",
    "        return ((y_actual-y_pred)**2).sum()/len(y_pred)\n",
    "  \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array_like, shape (n_samples, n_targets)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "        for i in range(self.iterations):\n",
    "            #predict y from y = mx + c\n",
    "            y_pred=self.predict(X)\n",
    "#             print(y_pred.shape)\n",
    "            #calculate loss\n",
    "#             loss = self.mse(y,y_pred)\n",
    "            loss = y_pred - y\n",
    "#             print(\"Loss \",loss)\n",
    "            #find gradient to minimize loss\n",
    "            gradient = X.T.dot(loss) / len(y)\n",
    "            cost = self.mse(y,y_pred)\n",
    "            if (cost<self.min):\n",
    "                self.min=cost\n",
    "#             print(cost)\n",
    "            #update coef\n",
    "            self.coef = self.coef - self.learning_rate * gradient\n",
    "#         print(\"The coefficients are:\")\n",
    "#         print(self.coef)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the linear model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "#         print(X.shape)\n",
    "#         print(self.coef.shape)\n",
    "#         print(X.dot(self.coef).shape)\n",
    "        return X.dot(self.coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code goes here\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data)\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "# find the mean of X\n",
    "x_mean= X.mean()\n",
    "y_mean= y.mean()\n",
    "# find the sd of X & y\n",
    "std_x = X.std()\n",
    "std_y = y.std()\n",
    "# normalization = (X - mean) / sd\n",
    "X = (X-x_mean)/std_x\n",
    "y = (y-y_mean)/std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 12), (152, 12), (354,), (152,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = linear_regression(0.01, 50, False, False, np.random.randint(10, size=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: \n",
      " 0    -0.273363\n",
      "1     4.247201\n",
      "2     1.481289\n",
      "3     3.906879\n",
      "4     0.294851\n",
      "5     5.095538\n",
      "6     6.112667\n",
      "7     6.278130\n",
      "8     2.792764\n",
      "9     5.770312\n",
      "10    1.570004\n",
      "11    6.241038\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"The coefficients are: \\n\", linear.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum squared error is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.41471683287571"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Minimum squared error is\")\n",
    "linear.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFlcvY_piKus"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum squared error and the corresponding set of column numbers are\n",
      "0.705582733941313\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "length_df=df.shape[1] - 1\n",
    "# print(length_df)\n",
    "columns_list = list(df.columns)[:length_df]\n",
    "# print(columns_list)\n",
    "min_min=9999\n",
    "combo = []\n",
    "\n",
    "for i in range(1,length_df+1):\n",
    "    cc = list(combinations(columns_list,i))\n",
    "    for c in cc:\n",
    "        c = list(c)\n",
    "#         print(c)\n",
    "        linear = linear_regression(0.01, 50, False, False, np.random.randint(10, size=len(c)))\n",
    "        linear.fit(X_train.iloc[:,c],y_train)\n",
    "        if(linear.min<min_min):\n",
    "            min_min=linear.min\n",
    "            combo = c\n",
    "\n",
    "print(\"The minimum squared error and the corresponding set of column numbers are\")\n",
    "print(min_min)            \n",
    "print(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. How do you interpret that a variable causes a model's mean square error to increase? (2 points)\n",
    "  - Answer: While calculating MSE, the two parameters that we need are the actual values and the predicted values and we find out the error in our predictions with respect to the actual values. A large MSE value signifies that the predicted data varies highly from the actual data. The variables or the parameters in our dataset might not be always tightly bound to the parameter we are trying to predict. Including some variables might even take us further away from the intended prediction. Hence, choosing a set of variables that give the least answer for Mean Square error gives better or more accurate predictions. Lesser the value of MSE, more tightly bound and dependent the predictions are on the chosen set of variables.If all the variables, including the ones which highly vary from the mean value are chosen, it will result in less accurate prediction.\n",
    "2. Why we would want to normalize our variables? (1 point)\n",
    "  - Answer: In a dataset, there are various kinds of variables with different scales of measures. Like for a housing dataset, area of the bedroom is a huge scale (100 Sq ft) whereas number of bedrooms is a single unit measure. Here, area of the bedroom will dominate the prediction if it is not normalized. By normalization, we scale the variables to solve this problem and make the learning process efficient. Once the variables are on the same scale, it is easier to handle such data and process it for predictions\n",
    "3. A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)\n",
    "  - Answer: In my opinion, the normalized data will not generate the exact coefficients as the original not normalized data. When we normalize data, there will be a corresponding change in the coefficients as well, as the coefficients for these scaled entries will be calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def generate_dataset(k0, k1):\n",
    "    #generate coefficients randomly\n",
    "    a = np.random.randint(-1000,1000)\n",
    "    b = np.random.randint(-1000,1000)\n",
    "    c = np.random.randint(1,500)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    #find x,y coordinates to plot the ax+by+c line\n",
    "    rx = np.random.randint(-500,500,size=30)\n",
    "    ry = -(a/b)*rx - (c/b)\n",
    "    plt.plot(rx,ry,'r-')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "        \n",
    "    #generating dataset\n",
    "    \n",
    "    while k0!=0:\n",
    "        temp_x = np.random.randint(-500,500)\n",
    "        temp_y = np.random.randint(-500,500) \n",
    "        res = a*temp_x + b*temp_y + c\n",
    "        if (res<0):\n",
    "            k0-=1\n",
    "            X.append((temp_x, temp_y))\n",
    "            y.append(-1)\n",
    "#     print(X[:5])\n",
    "            \n",
    "    while k1!=0:\n",
    "        temp_x = np.random.randint(-500,500)\n",
    "        temp_y = np.random.randint(-500,500) \n",
    "        res = a*temp_x + b*temp_y + c\n",
    "        if (res>0):\n",
    "            k1-=1\n",
    "            X.append((temp_x, temp_y))\n",
    "            y.append(1)\n",
    "#     checking for the class label of each point in order to assign colors to each and plot\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        res = a*X[i][0] + b*X[i][1] + c\n",
    "        if (res<0):\n",
    "            plt.plot(X[i][0], X[i][1], 'bo')\n",
    "        else:\n",
    "            plt.plot(X[i][0], X[i][1],'go')\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X,y = generate_dataset(20,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPk7AZaLkXSh"
   },
   "outputs": [],
   "source": [
    "def fit_line(X, y):\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if (y[i]== -1):\n",
    "            plt.plot(X[i][0],X[i][1],'bo')\n",
    "        else:\n",
    "            plt.plot(X[i][0],X[i][1],'go')\n",
    "    \n",
    "    a = np.random.randint(-500,100)\n",
    "    b = np.random.randint(0,1000)\n",
    "    c = np.random.randint(1,500)\n",
    "    \n",
    "    rx = np.random.randint(-500,500,size=30)\n",
    "    ry = -(a/b)*rx - (c/b)\n",
    "    \n",
    "    y_p = []\n",
    "    # end when all points are correctly classified\n",
    "    while y != y_p:\n",
    "        y_p = []\n",
    "        for i in range(len(X)):\n",
    "            res = a*X[i][0] + b*X[i][1] + c\n",
    "            if (res < 0):\n",
    "                y_p.append(-1)\n",
    "            else :\n",
    "                y_p.append(1)\n",
    "\n",
    "            if y[i] != y_p[i]:\n",
    "                a = a + y[i]*X[i][0]\n",
    "                b = b + y[i]*X[i][1]\n",
    "                ry = -(a/b)*rx - (c/b)\n",
    "        plt.plot(rx,ry,'r-')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "    \n",
    "\n",
    "    \"\"\"Predict using the binary classification model. Use the dataset generated \n",
    "    using generate_data() as input for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_samples, n_features)\n",
    "        Samples.\n",
    "    y : array_like, shape (n_labels, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (1,n_features)\n",
    "        Returns the final weight vector w.  \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_line(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
    "\n",
    "  - Answer: Linearly separable data is when we can classify the data points in a 2 dimensional XY plane by a simple line between the the 2 classes. Binary classification is possible for the model above because the data is linearly separable. If the data is not linearly separable, a simple line equation in the form ax + by + c = 0 cannot be plotted to classify data that is not linearly separable. For example, data which lies in concentric circles which can only be classified by another circle between the 2 classes. The equation ax + by + c = 0, only accounts for a simple line and  can only take a linear form with different values of X and Y. Hence a straight line can only classify our data points into 2 parts.   \n",
    "\n",
    "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
    "\n",
    "  - Answer: If the data is not linearly separable, this model cannot classify it into either of the class labels correctly as the classification is done by a straight line in our model. If the data is not linearly separable, in order to salvage the situation, we can remove the restriction of the classifying the data by a straight line and classify it by using non linear classification techniques like KNN. Depending on the distribution of the data, the suitable method for classification can be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

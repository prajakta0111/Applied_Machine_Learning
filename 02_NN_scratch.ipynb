{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tg9fRSjFtsRu"
   },
   "source": [
    "# Assignment #3\n",
    "## P556: Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjsZpx5C9eBH"
   },
   "source": [
    "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. In this assignment you will implement a fully-connected and a convolutional neural network from scratch. To simplify the implementation, we are asking you to implement static architectures, but you are free to support variable number of layers/neurons/activations/optimizers/etc. We recommend that you make use of private methods so you can easily troubleshoot small parts of your model as you develop them, instead of trying to figure out which parts are not working correctly after implementing everything. Also, keep in mind that there is code from your fully-connected neural network that can be re-used on the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NzW9M-btzqO"
   },
   "source": [
    "Problem #1.1 (40 points): Implement a fully-connected neural network from scratch. The neural network will have the following architecture:\n",
    "\n",
    "- Input layer\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
    "- Dropout with a value of 0.2\n",
    "- Output layer, using softmax as the activation function\n",
    "\n",
    "The model will use categorical crossentropy as its loss function. \n",
    "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
    "We will evaluate the model using accuracy.\n",
    "\n",
    "Why this architecture? We are trying to reproduce from scratch the following [example from the Keras documentation](https://keras.io/examples/mnist_mlp/). This means that you can compare your results by running the Keras code provided above to see if you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    expo = []\n",
    "    for val in x:\n",
    "        v = np.exp(val - np.max(val))\n",
    "        expo.append(v / v.sum())\n",
    "    return np.array(expo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_preds, y_train):\n",
    "    \n",
    "    y_preds = softmax(y_preds)\n",
    "    \n",
    "    # number of rows\n",
    "    len_out = y_preds.shape[0]\n",
    "    \n",
    "    # find actuals\n",
    "    y_train = y_train.astype(int)\n",
    "    y_train = np.argmax(y_train, axis = 1)\n",
    "    \n",
    "    # find the minimum non zero value\n",
    "    m = np.min(y_preds[np.nonzero(y_preds)])\n",
    "    \n",
    "    # replace all zeros with min non zero val\n",
    "    y_preds[y_preds == 0] = m\n",
    "    \n",
    "    nll = -np.log(y_preds[range(len_out), y_train])\n",
    "    nll = np.mean(nll)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def accuracy(y_preds, y_train):\n",
    "    y_train = np.argmax(y_train, axis = 1)\n",
    "    y_preds = np.argmax(y_preds, axis = 1)\n",
    "    return (np.sum(np.equal(y_preds, y_train)) / len(y_train)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradients(nn, x_train, y_train):\n",
    "    y_train = np.argmax(y_train, axis = 1)\n",
    "    m = y_train.shape[0]\n",
    "    grad = softmax(nn.layer3)\n",
    "    grad[range(m),y_train] -= 1\n",
    "    grad = grad/m\n",
    "\n",
    "\n",
    "    dout = grad\n",
    "    dw3 = np.dot(dout.T, nn.relu2)\n",
    "    db3 = np.sum(dout) / m\n",
    "    d_relu22 = dout.dot(nn.w3.T)\n",
    "    d_relu2 = d_relu22.copy()\n",
    "    d_relu2[d_relu2 < 0] = 0\n",
    "\n",
    "    dw2 = nn.u2.T * (np.dot(d_relu22.T, nn.relu1))\n",
    "    db2 = np.sum(d_relu22) / m\n",
    "    d_relu11 = d_relu2.dot(nn.w2)\n",
    "    d_relu1 = d_relu11.copy()\n",
    "    d_relu1[d_relu1 < 0] = 0\n",
    "    \n",
    "    dw1 = nn.u1.T * (np.dot(d_relu11.T, x_train))\n",
    "    db1 = np.sum(d_relu11) / m\n",
    "    return [dw1, dw2, dw3, db1, db2, db3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(model, gr, lr):\n",
    "    model.w1 -= gr[0].T * lr\n",
    "    model.w2 -= gr[1].T * lr\n",
    "    model.w3 -= gr[2].T * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rPUmRqBtpS2"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, epochs, learning_rate):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_in = 784\n",
    "        self.n_h = 512\n",
    "        self.n_out = 10\n",
    "        self.p = 0.8\n",
    "        self.w1 = np.random.randn(self.n_in,self.n_h)\n",
    "        self.b1 = np.zeros(self.n_h)\n",
    "        self.w2 = np.random.randn(self.n_h,self.n_h)\n",
    "        self.b2 = np.zeros(self.n_h)\n",
    "        self.w3 = np.random.randn(self.n_h,self.n_out)\n",
    "        self.b3 = np.zeros(self.n_out)\n",
    "        self.u1 = np.random.binomial(1, self.p, size=self.w1.shape)\n",
    "        self.u2 = np.random.binomial(1, self.p, size=self.w2.shape)\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.layer1 = np.dot(x, self.u1 * self.w1) + self.b1\n",
    "        self.relu1 = relu(self.layer1)\n",
    "        self.layer2 = np.dot(self.relu1, self.u2 * self.w2) + self.b2\n",
    "        self.relu2 = relu(self.layer2)\n",
    "        self.layer3 = np.dot(self.relu2,self.w3) + self.b3\n",
    "        return self.layer3\n",
    "        \n",
    "    def evaluate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(model,rho,g):\n",
    "    grad_squared1 = 0\n",
    "    grad_squared2 = 0\n",
    "    grad_squared3 = 0\n",
    "    grad_bias1 = 0\n",
    "    grad_bias2 = 0\n",
    "    grad_bias3 = 0\n",
    "    grad_squared1 = rho * grad_squared1 + (1-rho) * g[0] * g[0]\n",
    "    grad_bias1 = rho * grad_bias1 + (1-rho) * g[3] * g[3]\n",
    "    eps=1e-4\n",
    "    sq1 = np.sqrt(grad_squared1)\n",
    "    sq_1 = np.sqrt(grad_bias1)\n",
    "  \n",
    "    s1 = np.min(sq1[np.nonzero(sq1)])\n",
    "    sq1[sq1 == 0] = s1\n",
    "    temp1 = (model.learning_rate / sq1 + eps ) * g[0]\n",
    "    temp_1 = (model.learning_rate / sq_1 + eps) * g[3]\n",
    "    model.w1 -= temp1.T\n",
    "    model.b1 -= temp_1.T\n",
    "    \n",
    "    grad_squared2 = rho * grad_squared2 + (1-rho) * g[1] * g[1]\n",
    "    grad_bias2 = rho * grad_bias2 + (1-rho) * g[4] * g[4]\n",
    "    sq2 = np.sqrt(grad_squared2)\n",
    "    sq_2 = np.sqrt(grad_bias2)\n",
    "    s2 = np.min(sq2[np.nonzero(sq2)])\n",
    "    sq2[sq2 == 0] = s2\n",
    "    temp2 = (model.learning_rate / sq2 + eps) * g[1]\n",
    "    temp_2 = (model.learning_rate / sq_2 + eps) * g[4]\n",
    "    model.w2 -= temp2.T\n",
    "    model.b2 -= temp_2.T\n",
    "    \n",
    "    grad_squared3 = rho * grad_squared3 + (1-rho) * g[2] * g[2]\n",
    "    grad_bias3 = rho * grad_bias3 + (1-rho) * g[5] * g[5]\n",
    "    sq3 = np.sqrt(grad_squared3)\n",
    "    sq_3 = np.sqrt(grad_bias3)\n",
    "    s3 = np.min(sq3[np.nonzero(sq3)])\n",
    "    sq3[sq3 == 0] = s3\n",
    "    temp3 = (model.learning_rate / sq3 + eps) * g[2]\n",
    "    temp_3 = (model.learning_rate / sq_3 + eps) * g[5]\n",
    "    model.w3 -= temp3.T\n",
    "    model.b3 -= temp_3.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    params = {}\n",
    "    params['w1'] = model.w1\n",
    "    params['w2'] = model.w2\n",
    "    params['w3'] = model.w3\n",
    "    params['b1'] = model.b1\n",
    "    params['b2'] = model.b2\n",
    "    params['b3'] = model.b3\n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DH3bgJyPuE2O"
   },
   "source": [
    "Problem #1.2 (10 points): Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsN4sUoUugl8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
    "# library. Keras can be installed using pip: python -m pip install keras\n",
    "\n",
    "# Original source for the dataset:\n",
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "# Reference to the Fashion-MNIST's Keras function: \n",
    "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
    "\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float64')\n",
    "x_test = x_test.astype('float64')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "n = 60000\n",
    "def train(x_train, y_train):\n",
    "    model = NeuralNetwork(10,0.001)\n",
    "    loss = []\n",
    "    acc = []\n",
    "    m = 0\n",
    "    for i in range(50):\n",
    "        preds = model.fit(x_train)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss.append(cross_entropy(preds, y_train))\n",
    "        acc.append(accuracy(preds, y_train))\n",
    "#         print(\"Loss at iteration \", i,\"is\", loss[-1])\n",
    "#         print(\"Accuracy \", acc[-1])\n",
    "        if acc[-1] > m:\n",
    "            m = acc[-1]\n",
    "            p = save_model(model)\n",
    "            md = copy.deepcopy(model)\n",
    "            \n",
    "#             print(\"Better model found at epoch\", i , \"with accuracy \", acc[-1])\n",
    "\n",
    "        # calculate the gradients\n",
    "        g = calc_gradients(model, x_train, y_train)\n",
    "\n",
    "        # update weights\n",
    "        rms(model,0.9,g)\n",
    "#     print(\"Finished training for fold\")\n",
    "#     test_preds = model.fit(x_test)\n",
    "#     print(\"Test loss: \",cross_entropy(test_preds, y_test))\n",
    "#     print(accuracy(test_preds, y_test))\n",
    "#     test_preds = model()\n",
    "    return p, m, md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(folds = 5):\n",
    "    num_folds = 5\n",
    "    saved = {}\n",
    "    subset_size = int(len(x_train)/num_folds)\n",
    "#     print(subset_size)\n",
    "    for i in range(num_folds):\n",
    "        start_idx = i*subset_size\n",
    "        end_idx = start_idx + subset_size\n",
    "        test_idx = np.array([i for i in range(start_idx, end_idx)])\n",
    "#         print(test_idx[:5])\n",
    "        train_idx = np.array([i for i in range(len(x_train)) if i not in test_idx])\n",
    "        bm, ba, b_model = train(x_train[train_idx], y_train[train_idx])\n",
    "        saved[i] = [bm,ba]\n",
    "        print(\"Best training accuracy for fold \", i+1, \"is\" ,ba)\n",
    "        print(\"Finished training for fold\", i+1)\n",
    "        \n",
    "        \n",
    "        test_preds = b_model.fit(x_train[test_idx])\n",
    "        print(\"Fold Test loss: \",cross_entropy(test_preds, y_train[test_idx]))\n",
    "        print(\"Fold Test accuracy: \", accuracy(test_preds, y_train[test_idx]))\n",
    "        \n",
    "        test_preds1 = b_model.fit(x_test)\n",
    "        print(\"Actual Test data loss: \",cross_entropy(test_preds1, y_test))\n",
    "        print(\"Actual Test data accuracy: \",accuracy(test_preds1, y_test))\n",
    "        \n",
    "        print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training accuracy for fold  1 is 50.15625\n",
      "Finished training for fold 1\n",
      "Fold Test loss:  304.26719673638655\n",
      "Fold Test accuracy:  50.375\n",
      "Actual Test data loss:  312.958774858521\n",
      "Actual Test data accuracy:  49.57\n",
      "========================================\n",
      "Best training accuracy for fold  2 is 51.35833333333333\n",
      "Finished training for fold 2\n",
      "Fold Test loss:  304.0014301127699\n",
      "Fold Test accuracy:  50.83333333333333\n",
      "Actual Test data loss:  303.0186569970457\n",
      "Actual Test data accuracy:  51.129999999999995\n",
      "========================================\n",
      "Best training accuracy for fold  3 is 55.2875\n",
      "Finished training for fold 3\n",
      "Fold Test loss:  289.02220027450284\n",
      "Fold Test accuracy:  55.41666666666667\n",
      "Actual Test data loss:  294.73723514939365\n",
      "Actual Test data accuracy:  54.900000000000006\n",
      "========================================\n",
      "Best training accuracy for fold  4 is 51.260416666666664\n",
      "Finished training for fold 4\n",
      "Fold Test loss:  300.39982127812044\n",
      "Fold Test accuracy:  51.366666666666674\n",
      "Actual Test data loss:  305.24779693774093\n",
      "Actual Test data accuracy:  50.760000000000005\n",
      "========================================\n",
      "Best training accuracy for fold  5 is 46.14791666666667\n",
      "Finished training for fold 5\n",
      "Fold Test loss:  330.9166498689765\n",
      "Fold Test accuracy:  46.325\n",
      "Actual Test data loss:  340.01740293185816\n",
      "Actual Test data accuracy:  45.09\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "k_fold()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A3-P556-F19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
